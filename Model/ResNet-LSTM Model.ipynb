{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd255ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7570618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Image Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#2. Feature Extractor: ResNet\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.output_dim = resnet.fc.in_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_extractor(x).squeeze(-1).squeeze(-1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ab90b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. LSTM Classifier\n",
    "class VideoClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=feature_dim, hidden_size=hidden_dim,\n",
    "                                           num_layers=num_layers, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        out = self.classifier(hn[-1])\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f26d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Dataset Class\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, split_file, metadata_file, dota_dir, bdd_dir, sample_count=64):\n",
    "        with open(split_file_path, 'r') as f:\n",
    "            self.video_ids =  [line.strip() for line in f if line.strip()]\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "            \n",
    "        self.dota_dir = dota_dir\n",
    "        self.bdd_dir = bdd_dir\n",
    "        self.sample_count = sample_count\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_id = self.video_ids[idx]\n",
    "        label = 0 if self.metadata[video_id][\"anomaly_class\"] == \"normal\" else 1\n",
    "        folder = os.path.join(self.bdd_dir if label == 0 else self.dota_dir, video_id)\n",
    "        \n",
    "        frame_paths = sorted([os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.jpg')])\n",
    "        total = len(frame_paths)\n",
    "        step = max(total // self.sample_count, 1)\n",
    "        selected = [frame_paths[i * step] for i in range(self.sample_count)]\n",
    "        images = [transform(Image.open(f).convert('RGB')) for f in selected[:self.sample_count]]\n",
    "        \n",
    "        return torch.stack(images), torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11f1cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Evaluation\n",
    "def evaluate(model, loader, feature_extractor, device):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for frames, label in loader:\n",
    "            frames = frames.squeeze(0).to(device)\n",
    "            features = feature_extractor(frames)\n",
    "            sequence = features.unsqueeze(0)\n",
    "            output = model(sequence.to(device))\n",
    "            preds.append(int(output.item() > 0.5))\n",
    "            targets.append(int(label.item()))\n",
    "            \n",
    "    acc = accuracy_score(targets, preds)\n",
    "    prec = precision_score(targets, preds)\n",
    "    rec = recall_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds)\n",
    "    print(f\"Val â€” Acc: {acc:.4f}  Precision: {prec:.4f}  Recall: {rec:.4f}  F1: {f1:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53d662bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Training Loop\n",
    "def train(model, train_loader, val_loader, feature_extractor, device, epochs):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for frames, label in train_loader:\n",
    "            frames = frames.squeeze(0).to(device)\n",
    "            label = label.to(device)\n",
    "            features = feature_extractor(frames)\n",
    "            sequence = features.unsqueeze(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(sequence)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{epochs} -- Train Loss: {running_loss / len(train_loader): .4f}\")\n",
    "        evaluate(model, val_loader, feature_extractor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Inference\n",
    "if __name__ == '__main__':\n",
    "    #Paths\n",
    "    train_split_file = \"dataset/train_split_merged.txt\"\n",
    "    metadata_file = \"dataset/metadata_train_merged.json\"\n",
    "    dota_dir = \"DoTA/DoTA_Frames\"\n",
    "    bdd_dir = \"BDD100K/BDD_Frames\"\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Data\n",
    "    train_dataset = VideoFrameDataset(train_split, metadata_path, dota_dir, bdd_dir)\n",
    "    val_dataset = VideoFrameDataset(val_split, metadata_path, dota_dir, bdd_dir)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "    # Models\n",
    "    feature_extractor = ResNetFeatureExtractor().to(device)\n",
    "    classifier = VideoClassifier(feature_dim=feature_extractor.output_dim)\n",
    "\n",
    "    # Train\n",
    "    train(classifier, train_loader, val_loader, feature_extractor, device, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7552cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
